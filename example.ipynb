{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import tiktoken\n",
    "from loguru import logger\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import UnstructuredPowerPointLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# from streamlit_chat import message\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.memory import StreamlitChatMessageHistory\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(\n",
    "    page_title=\"DirChat\",\n",
    "    page_icon=\":books:\")\n",
    "\n",
    "    st.title(\"_Private Data :red[QA Chat]_ :books:\")\n",
    "\n",
    "    if \"conversation\" not in st.session_state:\n",
    "        st.session_state.conversation = None\n",
    "\n",
    "    if \"chat_history\" not in st.session_state:\n",
    "        st.session_state.chat_history = None\n",
    "\n",
    "    if \"processComplete\" not in st.session_state:\n",
    "        st.session_state.processComplete = None\n",
    "\n",
    "    with st.sidebar:\n",
    "        uploaded_files =  st.file_uploader(\"Upload your file\",type=['pdf','docx'],accept_multiple_files=True)\n",
    "        openai_api_key = st.text_input(\"OpenAI API Key\", key=\"chatbot_api_key\", type=\"password\")\n",
    "        process = st.button(\"Process\")\n",
    "    if process:\n",
    "        if not openai_api_key:\n",
    "            st.info(\"Please add your OpenAI API key to continue.\")\n",
    "            st.stop()\n",
    "        files_text = get_text(uploaded_files)\n",
    "        text_chunks = get_text_chunks(files_text)\n",
    "        vetorestore = get_vectorstore(text_chunks)\n",
    "     \n",
    "        st.session_state.conversation = get_conversation_chain(vetorestore,openai_api_key) \n",
    "\n",
    "        st.session_state.processComplete = True\n",
    "\n",
    "    if 'messages' not in st.session_state:\n",
    "        st.session_state['messages'] = [{\"role\": \"assistant\", \n",
    "                                        \"content\": \"안녕하세요! 주어진 문서에 대해 궁금하신 것이 있으면 언제든 물어봐주세요!\"}]\n",
    "\n",
    "    for message in st.session_state.messages:\n",
    "        with st.chat_message(message[\"role\"]):\n",
    "            st.markdown(message[\"content\"])\n",
    "\n",
    "    history = StreamlitChatMessageHistory(key=\"chat_messages\")\n",
    "\n",
    "    # Chat logic\n",
    "    if query := st.chat_input(\"질문을 입력해주세요.\"):\n",
    "        st.session_state.messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "        with st.chat_message(\"user\"):\n",
    "            st.markdown(query)\n",
    "\n",
    "        with st.chat_message(\"assistant\"):\n",
    "            chain = st.session_state.conversation\n",
    "\n",
    "            with st.spinner(\"Thinking...\"):\n",
    "                result = chain({\"question\": query})\n",
    "                with get_openai_callback() as cb:\n",
    "                    st.session_state.chat_history = result['chat_history']\n",
    "                response = result['answer']\n",
    "                source_documents = result['source_documents']\n",
    "\n",
    "                st.markdown(response)\n",
    "                with st.expander(\"참고 문서 확인\"):\n",
    "                    st.markdown(source_documents[0].metadata['source'], help = source_documents[0].page_content)\n",
    "                    st.markdown(source_documents[1].metadata['source'], help = source_documents[1].page_content)\n",
    "                    st.markdown(source_documents[2].metadata['source'], help = source_documents[2].page_content)\n",
    "                    \n",
    "\n",
    "\n",
    "# Add assistant message to chat history\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def get_text(docs):\n",
    "\n",
    "    doc_list = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        file_name = doc.name  # doc 객체의 이름을 파일 이름으로 사용\n",
    "        with open(file_name, \"wb\") as file:  # 파일을 doc.name으로 저장\n",
    "            file.write(doc.getvalue())\n",
    "            logger.info(f\"Uploaded {file_name}\")\n",
    "        if '.pdf' in doc.name:\n",
    "            loader = PyPDFLoader(file_name)\n",
    "            documents = loader.load_and_split()\n",
    "        elif '.docx' in doc.name:\n",
    "            loader = Docx2txtLoader(file_name)\n",
    "            documents = loader.load_and_split()\n",
    "        elif '.pptx' in doc.name:\n",
    "            loader = UnstructuredPowerPointLoader(file_name)\n",
    "            documents = loader.load_and_split()\n",
    "\n",
    "        doc_list.extend(documents)\n",
    "    return doc_list\n",
    "\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=900,\n",
    "        chunk_overlap=100,\n",
    "        length_function=tiktoken_len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_vectorstore(text_chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "                                        model_name=\"jhgan/ko-sroberta-multitask\",\n",
    "                                        model_kwargs={'device': 'cpu'},\n",
    "                                        encode_kwargs={'normalize_embeddings': True}\n",
    "                                        )  \n",
    "    vectordb = FAISS.from_documents(text_chunks, embeddings)\n",
    "    return vectordb\n",
    "\n",
    "def get_conversation_chain(vetorestore,openai_api_key):\n",
    "    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name = 'gpt-3.5-turbo',temperature=0)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm, \n",
    "            chain_type=\"stuff\", \n",
    "            retriever=vetorestore.as_retriever(search_type = 'mmr', vervose = True), \n",
    "            memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),\n",
    "            get_chat_history=lambda h: h,\n",
    "            return_source_documents=True,\n",
    "            verbose = True\n",
    "        )\n",
    "\n",
    "    return conversation_chain\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        max_tokens=2048,\n",
    "        temperature=0.5,\n",
    ")\n",
    "\n",
    "# prmpt를 PromptTemplate 객체로 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    너는 기업 면접관이야.\n",
    "    {resume} 에 대해서 6가지의 면접 질문을 날카롭게 생성해줘.\n",
    "    \"\"\"\n",
    ")\n",
    "# prompt 객체와 model 객체릴 파이프(|) 연산자로 연결하고\n",
    "# invoke 메서드를 이용하여 input을 전달합니다.\n",
    "# 이를 통해 AI모델이 생성한 메세지를 반환합니다.\n",
    "chain = prompt | llm\n",
    "chain.invoke({\"resume\":vector_db})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hydralit_components\n",
      "  Using cached hydralit_components-1.0.10.tar.gz (20.1 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: streamlit>=1.7 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from hydralit_components) (1.35.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from hydralit_components) (5.2.2)\n",
      "Collecting bs4 (from hydralit_components)\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: altair<6,>=4.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (5.3.0)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (1.8.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (8.1.7)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (1.26.4)\n",
      "Requirement already satisfied: packaging<25,>=16.8 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (24.0)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (2.2.2)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (10.3.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (4.25.3)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (16.1.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (2.32.3)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (8.3.0)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (4.12.1)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (3.1.43)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (6.4)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from streamlit>=1.7->hydralit_components) (4.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from bs4->hydralit_components) (4.12.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from altair<6,>=4.0->streamlit>=1.7->hydralit_components) (3.1.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from altair<6,>=4.0->streamlit>=1.7->hydralit_components) (4.22.0)\n",
      "Requirement already satisfied: toolz in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from altair<6,>=4.0->streamlit>=1.7->hydralit_components) (0.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from click<9,>=7.0->streamlit>=1.7->hydralit_components) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.7->hydralit_components) (4.0.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit>=1.7->hydralit_components) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit>=1.7->hydralit_components) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from pandas<3,>=1.3.0->streamlit>=1.7->hydralit_components) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from requests<3,>=2.27->streamlit>=1.7->hydralit_components) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from requests<3,>=2.27->streamlit>=1.7->hydralit_components) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from requests<3,>=2.27->streamlit>=1.7->hydralit_components) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from requests<3,>=2.27->streamlit>=1.7->hydralit_components) (2024.6.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from rich<14,>=10.14.0->streamlit>=1.7->hydralit_components) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from rich<14,>=10.14.0->streamlit>=1.7->hydralit_components) (2.18.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from beautifulsoup4->bs4->hydralit_components) (2.5)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=1.7->hydralit_components) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from jinja2->altair<6,>=4.0->streamlit>=1.7->hydralit_components) (2.1.5)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.7->hydralit_components) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.7->hydralit_components) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.7->hydralit_components) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit>=1.7->hydralit_components) (0.18.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit>=1.7->hydralit_components) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\joseu\\anaconda3\\envs\\streamlit\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit>=1.7->hydralit_components) (1.16.0)\n",
      "Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Building wheels for collected packages: hydralit_components\n",
      "  Building wheel for hydralit_components (setup.py): started\n",
      "  Building wheel for hydralit_components (setup.py): still running...\n",
      "  Building wheel for hydralit_components (setup.py): still running...\n",
      "  Building wheel for hydralit_components (setup.py): still running...\n",
      "  Building wheel for hydralit_components (setup.py): finished with status 'done'\n",
      "  Created wheel for hydralit_components: filename=hydralit_components-1.0.10-py3-none-any.whl size=24347449 sha256=52f9327a4e0b7eec4ecdeb6d85e57a2497ff9c5b685438b973410017cc6046ef\n",
      "  Stored in directory: c:\\users\\joseu\\appdata\\local\\pip\\cache\\wheels\\5d\\5b\\e9\\3b7a3a5f9f470fd6745417860bc8dc7a68fd6862b19a9290cd\n",
      "Successfully built hydralit_components\n",
      "Installing collected packages: bs4, hydralit_components\n",
      "Successfully installed bs4-0.0.2 hydralit_components-1.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install hydralit_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "분할된 청크의수: 1\n",
      "['\"최고가 되기 위해서\"\\n금융결제원은 국내 다양한 금융 관련 서비스를 지원하는 최고의 기업입니다.\\n이런 여러 서비스 운영에 있어 중요한 것은 사용자에게 적절한 UI의 표출, 그리고 효율적인 데이터 처리를 통한 빠른 속도 제공입니다.\\n저는 이를 배우기 위해 매일 진행 중인 알고리즘 풀이 외에도 기존에 공부했던 JSP에 대해 조금 더 파악하고, spring에 활용하기 전에 배우기 위해 관련 학원에 다니며 공부하였고, 여러 예제를 사용한 Spring 프로젝트를 진행했습니다.\\n그리고 JPA를 사용하는 여러 방식과 주의점, querydsl 등 많은 부분에의 공부를 해 보았습니다.\\n백엔드 이외에도 front-end에 활용되는 javascript의 기능 활용을 더 잘 알기 위해, vanilla JS를 사용하여 조금씩 난도를 높여가며 프로젝트를 진행 중입니다.\\n그렇게 실력을 쌓아 가던 중 운이 좋게도 그동안 제가 공부하였고 앞으로의 진로로 결정하게 된 java spring이 기반이 되는 전산직 사무원을 채용한다는 소식을 들었고, 최고의 기업에서 가장 좋아하는 공부를 할 수 있을 것이라는 기대감을 품고 지원하게 되었습니다.']\n",
      "\"최고가 되기 위해서\"\n",
      "금융결제원은 국내 다양한 금융 관련 서비스를 지원하는 최고의 기업입니다.\n",
      "이런 여러 서비스 운영에 있어 중요한 것은 사용자에게 적절한 UI의 표출, 그리고 효율적인 데이터 처리를 통한 빠른 속도 제공입니다.\n",
      "저는 이를 배우기 위해 매일 진행 중인 알고리즘 풀이 외에도 기존에 공부했던 JSP에 대해 조금 더 파악하고, spring에 활용하기 전에 배우기 위해 관련 학원에 다니며 공부하였고, 여러 예제를 사용한 Spring 프로젝트를 진행했습니다.\n",
      "그리고 JPA를 사용하는 여러 방식과 주의점, querydsl 등 많은 부분에의 공부를 해 보았습니다.\n",
      "백엔드 이외에도 front-end에 활용되는 javascript의 기능 활용을 더 잘 알기 위해, vanilla JS를 사용하여 조금씩 난도를 높여가며 프로젝트를 진행 중입니다.\n",
      "그렇게 실력을 쌓아 가던 중 운이 좋게도 그동안 제가 공부하였고 앞으로의 진로로 결정하게 된 java spring이 기반이 되는 전산직 사무원을 채용한다는 소식을 들었고, 최고의 기업에서 가장 좋아하는 공부를 할 수 있을 것이라는 기대감을 품고 지원하게 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = \"sk-proj-dda7JIkccOeN1l6TtwOtT3BlbkFJWvKSdCKCSzHx4RQgxKKi\"\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "splitted_text = text_splitter.split_text(\n",
    "\"\"\"\n",
    "\"최고가 되기 위해서\"\n",
    "금융결제원은 국내 다양한 금융 관련 서비스를 지원하는 최고의 기업입니다.\n",
    "이런 여러 서비스 운영에 있어 중요한 것은 사용자에게 적절한 UI의 표출, 그리고 효율적인 데이터 처리를 통한 빠른 속도 제공입니다.\n",
    "저는 이를 배우기 위해 매일 진행 중인 알고리즘 풀이 외에도 기존에 공부했던 JSP에 대해 조금 더 파악하고, spring에 활용하기 전에 배우기 위해 관련 학원에 다니며 공부하였고, 여러 예제를 사용한 Spring 프로젝트를 진행했습니다.\n",
    "그리고 JPA를 사용하는 여러 방식과 주의점, querydsl 등 많은 부분에의 공부를 해 보았습니다.\n",
    "백엔드 이외에도 front-end에 활용되는 javascript의 기능 활용을 더 잘 알기 위해, vanilla JS를 사용하여 조금씩 난도를 높여가며 프로젝트를 진행 중입니다.\n",
    "그렇게 실력을 쌓아 가던 중 운이 좋게도 그동안 제가 공부하였고 앞으로의 진로로 결정하게 된 java spring이 기반이 되는 전산직 사무원을 채용한다는 소식을 들었고, 최고의 기업에서 가장 좋아하는 공부를 할 수 있을 것이라는 기대감을 품고 지원하게 되었습니다.\n",
    "\"\"\"\n",
    ")\n",
    "print(f\"분할된 청크의수: {len(splitted_text)}\")\n",
    "print(splitted_text)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "db = Chroma.from_texts(splitted_text, embeddings)\n",
    "\n",
    "# 질의합니다.\n",
    "query = \"너는 무엇을 했어?\"\n",
    "docs = db.similarity_search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 당신이 최고의 기업이 되기 위해 어떤 노력을 해왔습니까?\n",
      "2. 어떤 기술을 배우기 위해 노력했고, 어떤 프로젝트를 진행해 보았습니까?\n",
      "3. 자바 스프링을 기반으로 하는 전산직 사무원으로 채용되기 위해 어떤 기대를 갖고 있습니까?\n",
      "4. 당신이 지원한 최고의 기업에서 어떤 분야에서 더 공부하고 싶은가요?\n",
      "5. UI의 표출과 데이터 처리 속도를 높이기 위해 어떤 노력을 기울였습니까?\n"
     ]
    }
   ],
   "source": [
    "def mk_questions(context, resume):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    splitted_text = text_splitter.split_text(resume)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    db = Chroma.from_texts(splitted_text, embeddings)\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.5, openai_api_key=\"sk-proj-dda7JIkccOeN1l6TtwOtT3BlbkFJWvKSdCKCSzHx4RQgxKKi\")\n",
    "\n",
    "    output_parser = CommaSeparatedListOutputParser()\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template = '''\n",
    "    You are an expert AI interviewer.\n",
    "    Use the given context to generate different predictive interview questions in Korean as many you can.\n",
    "    Please answer like\n",
    "    1.....\n",
    "    2.....\n",
    "    3.....\n",
    "    4.....\n",
    "    Please only generate the questions, don't add any explanations.\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    {format_instructions}\n",
    "    '''\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "    )\n",
    "\n",
    "    qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    retriever = db.as_retriever(\n",
    "        search_type='similarity',\n",
    "        search_kwargs={\n",
    "            'k': 3,  # Select top k search results\n",
    "        }\n",
    "    )\n",
    "\n",
    "    rag_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "    result = rag_chain.invoke(dict(input=context))\n",
    "    return result[\"answer\"]\n",
    "\n",
    "result = mk_questions(\"머신러닝\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "\n",
    "def mk_questions(context, resume):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "    splitted_text = text_splitter.split_text(resume)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "    db = Chroma.from_texts(splitted_text, embeddings)\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.5, openai_api_key=\"sk-proj-dda7JIkccOeN1l6TtwOtT3BlbkFJWvKSdCKCSzHx4RQgxKKi\")\n",
    "\n",
    "    output_parser = CommaSeparatedListOutputParser()\n",
    "    format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "    template = '''\n",
    "    You are an expert AI interviewer.\n",
    "    Use the given context to generate different predictive interview questions in Korean as many you can.\n",
    "    Please answer like\n",
    "    1.....\n",
    "    2.....\n",
    "    3.....\n",
    "    4.....\n",
    "    Please only generate the questions, don't add any explanations.\n",
    "\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\n",
    "    {format_instructions}\n",
    "    '''\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=template,\n",
    "        input_variables=[\"context\"],\n",
    "        partial_variables={\"format_instructions\": format_instructions},\n",
    "    )\n",
    "\n",
    "    qa_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "    retriever = db.as_retriever(\n",
    "        search_type='similarity',\n",
    "        search_kwargs={\n",
    "            'k': 3,  # Select top k search results\n",
    "        }\n",
    "    )\n",
    "\n",
    "    rag_chain = create_retrieval_chain(retriever, qa_chain)\n",
    "    result = rag_chain.invoke(dict(input=context))\n",
    "    return result[\"answer\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "streamlit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
